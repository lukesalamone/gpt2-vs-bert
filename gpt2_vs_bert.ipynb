{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2_vs_bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AysesTUIR5ZG"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyHYzjXKO6sC"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzeKKbwyKaBn"
      },
      "source": [
        "from transformers import OpenAIGPTTokenizer, GPT2LMHeadModel, GPT2Tokenizer, BertTokenizer, BertForMaskedLM, logging\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "import torch\n",
        "from torch import tensor\n",
        "from torch.nn.functional import softmax\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logging.set_verbosity_error()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_bWv_3CKfS0"
      },
      "source": [
        "wikitext2 = load_dataset('wikitext', 'wikitext-2-v1')\n",
        "wikitext2 = [x['text'].strip() for x in wikitext2['test']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSB6MxfJK5Ge"
      },
      "source": [
        "# functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_lkXmxlK9Il"
      },
      "source": [
        "# generate sequences with random length from 1-100\n",
        "def get_gpt2_sequences(data, batch_size=256, seq_len=100):\n",
        "    response = []\n",
        "        \n",
        "    tokenized = [tokenizer(x)['input_ids'] for x in data]\n",
        "    tokenized = [x for x in tokenized if len(x) >= seq_len]\n",
        "        \n",
        "    while len(response) < batch_size:\n",
        "        # pick a random line\n",
        "        line = tokenized[randint(len(tokenized))]\n",
        "\n",
        "        # get random start position\n",
        "        start = 0 if len(line) == seq_len else randint(len(line) - seq_len)\n",
        "        end = start + seq_len\n",
        "        \n",
        "        window = line[start:end]\n",
        "        predict_index = randint(seq_len)\n",
        "        replaced = window[predict_index]\n",
        "        window = window[0:predict_index]\n",
        "        \n",
        "        if len(window) == 0:\n",
        "            continue\n",
        "        \n",
        "        window, attention_mask = pad_sequence(window, seq_len)\n",
        "        \n",
        "        r = {\n",
        "            'context_length': predict_index,\n",
        "            'window': window,\n",
        "            'replaced_token': replaced,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "        \n",
        "        response.append(r)\n",
        "    return response"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35HHc2ozSrHb"
      },
      "source": [
        "def get_bert_sequences(data, batch_size=256, seq_len=100):\n",
        "  MASK_TOKEN = 103\n",
        "  response = []\n",
        "\n",
        "  tokenized = [tokenizer(x)['input_ids'] for x in data]\n",
        "  tokenized = [x for x in tokenized if len(x) >= seq_len]\n",
        "\n",
        "  while len(response) < batch_size:\n",
        "    # pick a random line\n",
        "    line = tokenized[randint(len(tokenized))]\n",
        "\n",
        "    start = 0 if len(line) == seq_len else randint(len(line) - seq_len)\n",
        "    end = start + seq_len\n",
        "\n",
        "    window = line[start:end]\n",
        "    predict_index = randint(seq_len)\n",
        "    actual_token = window[predict_index]\n",
        "    window[predict_index] = MASK_TOKEN\n",
        "\n",
        "    r = {\n",
        "      'window': window,\n",
        "      'replaced_token': actual_token,\n",
        "      'position': predict_index\n",
        "    }\n",
        "    \n",
        "    response.append(r)\n",
        "  return response"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ps5CGQ_LAwl"
      },
      "source": [
        "def predict_gpt2(batch, model):\n",
        "    actual = [x['replaced_token'] for x in batch]\n",
        "    context_lengths = [x['context_length'] for x in batch]\n",
        "    context = { 'input_ids': tensor([x['window'] for x in batch]),\n",
        "                'attention_mask': tensor([x['attention_mask'] for x in batch])\n",
        "                }\n",
        "    with torch.no_grad():\n",
        "      predicted = model.generate(**context, max_length=1)\n",
        "      predicted = [x[-1].item() for x in predicted]\n",
        "    \n",
        "    return predicted, actual, context_lengths"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az5Y5W_f_IVL"
      },
      "source": [
        "def predict_bert(batch, model):\n",
        "  actual = [x['replaced_token'] for x in batch]\n",
        "  positions = [x['position'] for x in batch]\n",
        "  position_mask = tensor([[x] for x in positions]).to('cuda')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    predictions = bert(input_ids=tensor([x['window'] for x in batch]).to('cuda'))\n",
        "    predictions = predictions[0].argmax(dim=-1)\n",
        "    predictions = [x.item() for x in predictions.gather(1, position_mask)]\n",
        "\n",
        "  return predictions, actual, positions"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKZqifPkLCud"
      },
      "source": [
        "def pad_sequence(sequence, length=100, pad_token=50256):\n",
        "    pad_len = length - len(sequence)\n",
        "    padding = [pad_token] * pad_len\n",
        "    attention_mask = [0] * pad_len + [1 for x in sequence]\n",
        "    \n",
        "    padded = padding + sequence\n",
        "    return padded, attention_mask"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlcgP1VeNPP_"
      },
      "source": [
        "def evaluate(model, sequence_getter, predict_fn, batch_size=256):\n",
        "  all_predicted, all_actual, all_context_lengths = [], [], []\n",
        "  epoch = 0\n",
        "  while len(all_predicted) < 10_000:\n",
        "    epoch += 1\n",
        "    batch = sequence_getter(wikitext2, batch_size)\n",
        "    predicted, actual, context_lengths = predict_fn(batch, model)\n",
        "    all_predicted += predicted\n",
        "    all_actual += actual\n",
        "    all_context_lengths += context_lengths\n",
        "\n",
        "    print('\\r', f'epoch {epoch}\\trunning accuracy: {np.mean([1 if x==y else 0 for x,y in zip(all_predicted, all_actual)])}', end='')\n",
        "\n",
        "  return all_predicted, all_actual, all_context_lengths"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB7r76iaLJb0"
      },
      "source": [
        "# gpt2 evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0p6SHCsOLA8",
        "outputId": "0a6ab126-d0b3-4899-9397-725e5a9f268a"
      },
      "source": [
        "evaluation = {}\n",
        "\n",
        "gpt_models = [('gpt2', 256), ('gpt2-medium', 128)]\n",
        "\n",
        "for model_name, batch_size in gpt_models:\n",
        "  print(f'{\"#\"*10} {model_name} {\"#\"*10}')\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  tokenizer.padding_side = \"left\"\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name).to('cuda')\n",
        "  predicted, actual, lengths = evaluate(model, get_gpt2_sequences, predict_gpt2, batch_size)\n",
        "  evaluation[model_name] = {'predicted': predicted, 'actual': actual, 'pos': lengths}\n",
        "\n",
        "del model"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " epoch 40\trunning accuracy: 0.3212890625"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciZMPL3sbf2W"
      },
      "source": [
        "# bert evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfOnZ3WuOGsv",
        "outputId": "88bba4e4-a910-46ae-be83-659182494e3a"
      },
      "source": [
        "bert_models = [('bert-base-cased', 256), ('bert-base-cased', 256), ('bert-large-cased', 128), ('bert-large-uncased', 128)]\n",
        "\n",
        "for model_name, batch_size in bert_models:\n",
        "  tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.padding_side = \"left\"\n",
        "  model = BertForMaskedLM.from_pretrained(model_name).to('cuda')\n",
        "  predicted, actual, mask_positions = evaluate(model, get_bert_sequences, predict_bert)\n",
        "  evaluation[model_name] = {'predicted':predicted, 'actual':actual, 'mask_positions': mask_positions}\n",
        "\n",
        "del model"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " epoch 40\trunning accuracy: 0.63720703125"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-HNCiHStZ0m"
      },
      "source": [
        "# analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gUBiAqXtdD4"
      },
      "source": [
        "accuracies = {}\n",
        "models = evaluation.keys()\n",
        "for m in models:\n",
        "    model_data = data[m]\n",
        "    model_type = data[m].pop('type', None)\n",
        "    model_accuracy = [[] for x in range(100)]\n",
        "\n",
        "    for pred, act, length in [a for a in zip(*model_data.values())]:\n",
        "        model_accuracy[length].append(1 if pred == act else 0)\n",
        "    accuracies[m] = [np.mean(x) if len(x) > 0 else 0 for x in model_accuracy]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqY9WkHAwoxV"
      },
      "source": [
        "import json\n",
        "with open('eval.json', 'w') as f:\n",
        "  json.dump(accuracies, f)"
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}